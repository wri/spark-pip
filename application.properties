# name that we'll use for this app - could be anything
spark.app.name=YARN Points in World

# set the size of the grid we'll use to split up the tasks
# we don't usually modify this, but may need to if we're hitting memory errors
reduce.size=0.5

# first let's add the source of our point data
points.path=s3://gfw2-data/alerts-tsv/loss_2017/

# and specify the X and Y fields
points.x=0
points.y=1

# and the fields we want to save from the points 
# for loss, this is likely loss year / area / tree cover density / biomass
points.fields=2,3,4,5

# now our source polygon datasets - can be a single TSV or a folder on S3
polygons.path=s3://gfw2-data/alerts-tsv/country-pages/ten-by-ten-gadm36/

# the index of the geometry field is always 0
polygons.wkt=0

# we want to save a bunch of other fields from the polygons as well
# boundary IDs, iso/adm1/adm2, etc
polygons.fields=1,2,3,4,5,6,7,8

# optional parameter to postprocess our output point + polygon table
# current options are loss, gain, extent, biomass, fire and glad
# as defined in src/main/scala/com/esri/Summarize.scala
# these functions take a dataframe and group by particular properties,
# writing a much smaller CSV (dropping lat/long, etc) to the output location
analysis.type=loss

# where the output will be stored on the cluster filesystem
# dont change this - our code will copy up to S3 later
output.path=hdfs:///user/hadoop/output

# output sep is a comma
output.sep=,
